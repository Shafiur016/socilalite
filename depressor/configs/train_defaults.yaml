training:
  epochs: 10
  batch_size: 16
  early_stopping: true
  patience: 2
  gradient_accumulation: 1
optimizer:
  name: AdamW
  lr: 2e-5
  weight_decay: 0.01
scheduler:
  warmup_steps: 100
  max_steps: 1000
